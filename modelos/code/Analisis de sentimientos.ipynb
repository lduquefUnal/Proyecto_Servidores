{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc5830e",
   "metadata": {},
   "source": [
    "Utilizando el dataset \" Twitter Sentiment Analysis in Spanish Tweets\", deberás\n",
    "implementar y comparar diferentes técnicas de Inteligencia Artificial y Modelos de\n",
    "Lenguaje (LLM) para la clasificación de sentimientos en comentarios de usuarios.\n",
    "• Demostrar conocimientos prácticos en técnicas de IA/ML\n",
    "• Evaluar capacidad de prompt engineering con LLMs\n",
    "• Analizar críticamente los resultados obtenidos\n",
    "• Muestra a utilizar: Los primeros 100 datos seleccionados aleatoriamente\n",
    "• Etiquetas esperadas: POSITIVO, NEGATIVO, NEUTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5efa3",
   "metadata": {},
   "source": [
    "\n",
    "## Plan de reentrenamiento reproducible\n",
    "1. Preparar entorno limpio (usa `venv` o `conda`) e instala solo las dependencias necesarias (`pandas`, `numpy`, `scikit-learn`, `joblib`, `tarfile` ya viene con Python). Mantén la misma versión de Python que usarás en `deploy.ipynb`.\n",
    "2. Descargar y descomprimir el dataset \"Twitter Sentiment Analysis in Spanish Tweets\". Si vuelves a muestrear, fija la semilla para poder replicar (ej. `random_state=42`).\n",
    "3. Limpieza y preprocesado: normaliza texto, elimina duplicados si aparecen y mapea las etiquetas a `POSITIVO`, `NEGATIVO`, `NEUTRO` como se usa en las métricas.\n",
    "4. División de datos: separa en train/test (y valid si lo necesitas) manteniendo estratificación para no perder la proporción de clases.\n",
    "5. Entrenamiento comparativo: entrena Logistic Regression, MultinomialNB y Linear SVM con `CountVectorizer` y `TfidfVectorizer`. Guarda las métricas (accuracy, f1-macro y por clase) en tablas reproducibles.\n",
    "6. Interpretación: documenta por qué eliges el modelo final (actualmente Linear SVM + CountVectorizer). Conserva también el `vectorizer` para inferencia.\n",
    "7. Serialización: guarda el pipeline completo (`joblib.dump`) y arma `model.tar.gz` con el artefacto y el script de inferencia usado por `deploy.ipynb`.\n",
    "8. Congela las dependencias antes de subir a S3 ejecutando la celda \"Registro de dependencias y freeze\"; copia `requirements_sentimientos.txt` junto al `model.tar.gz` para SageMaker y evita errores de versión en CloudWatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047f467",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06cbee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.14.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#%pip install pandas scikit-learn jupyter ipywidgets\n",
    "\n",
    "#%pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentiment_analysis_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorías en 'emotion':\", df['emotion'].unique())\n",
    "print(\"Categorías en 'sentiment':\", df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios de mapeo\n",
    "emotion_map = {\n",
    "    'overwhelmed': 'NEGATIVO', 'embarrassed': 'NEGATIVO', 'jealous': 'NEGATIVO',\n",
    "    'irritated': 'NEGATIVO', 'frustrated': 'NEGATIVO', 'distant': 'NEGATIVO',\n",
    "    'stupid': 'NEGATIVO', 'isolated': 'NEGATIVO', 'sleepy': 'NEGATIVO',\n",
    "\n",
    "    'responsive': 'NEUTRO', 'relaxed': 'NEUTRO',\n",
    "\n",
    "    'loving': 'POSITIVO', 'thankful': 'POSITIVO', 'secure': 'POSITIVO',\n",
    "    'confident': 'POSITIVO', 'successful': 'POSITIVO', 'surprised': 'POSITIVO',\n",
    "    'playful': 'POSITIVO', 'optimistic': 'POSITIVO', 'daring': 'POSITIVO'\n",
    "}\n",
    "\n",
    "sentiment_map = {\n",
    "    'scared': 'NEGATIVO', 'mad': 'NEGATIVO', 'sad': 'NEGATIVO',\n",
    "    'peaceful': 'NEUTRO',\n",
    "    'powerful': 'POSITIVO', 'joyful': 'POSITIVO'\n",
    "}\n",
    "\n",
    "# Combinar ambos mapas para una sola columna final\n",
    "def combine_sentiment(row):\n",
    "    e = emotion_map.get(row['emotion'], 'NEUTRO')\n",
    "    s = sentiment_map.get(row['sentiment'], 'NEUTRO')\n",
    "    # Regla: si alguno es NEGATIVO => NEGATIVO; si alguno es POSITIVO => POSITIVO\n",
    "    if 'NEGATIVO' in (e, s):\n",
    "        return 'NEGATIVO'\n",
    "    elif 'POSITIVO' in (e, s):\n",
    "        return 'POSITIVO'\n",
    "    else:\n",
    "        return 'NEUTRO'\n",
    "\n",
    "df['sentiment_label'] = df.apply(combine_sentiment, axis=1)\n",
    "\n",
    "# Verifica el resultado\n",
    "print(df[['text', 'emotion', 'sentiment', 'sentiment_label']].sample(10))\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "# usar dataset completo\n",
    "\n",
    "df_sample = df.copy()  # usar todo el dataset\n",
    "print(df_sample['sentiment_label'].value_counts())\n",
    "\n",
    "\n",
    "# Selecciona el texto y la etiqueta\n",
    "X = df_sample['text']\n",
    "y = df_sample['sentiment_label']\n",
    "\n",
    "# Divide en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Prueba ambas vectorizaciones\n",
    "vectorizers = {\n",
    "    \"CountVectorizer\": CountVectorizer(),\n",
    "    \"TfidfVectorizer\": TfidfVectorizer()\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Crear directorios para guardar los modelos\n",
    "os.makedirs('models/svm_countvectorizer', exist_ok=True)\n",
    "os.makedirs('models/svm_tfidfvectorizer', exist_ok=True)\n",
    "\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n--- Usando {vec_name} ---\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nModelo: {model_name}\")\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=['POSITIVO', 'NEGATIVO', 'NEUTRO'])\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1-score (macro): {f1:.4f}\")\n",
    "        print(\"Matriz de confusión:\")\n",
    "        print(cm)\n",
    "        print(\"Reporte de clasificación:\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "        # Guardar solo los modelos Linear SVM y sus vectorizadores\n",
    "        if model_name == \"Linear SVM\":\n",
    "            if vec_name == \"CountVectorizer\":\n",
    "                joblib.dump(model, 'models/svm_countvectorizer/model.joblib')\n",
    "                joblib.dump(vectorizer, 'models/svm_countvectorizer/vectorizer.joblib')\n",
    "            elif vec_name == \"TfidfVectorizer\":\n",
    "                joblib.dump(model, 'models/svm_tfidfvectorizer/model.joblib')\n",
    "                joblib.dump(vectorizer, 'models/svm_tfidfvectorizer/vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fccf7",
   "metadata": {},
   "source": [
    "## Interpretación de resultados y modelo recomendado\n",
    "Se compararon tres modelos de clasificación de sentimientos (Logistic Regression, Naive Bayes y Linear SVM) usando dos técnicas de vectorización de texto (CountVectorizer y TfidfVectorizer).\n",
    "- **CountVectorizer**: Linear SVM obtuvo la mejor precisión (Accuracy: 0.82, F1 macro: 0.77), mostrando buen desempeño en las clases POSITIVO y NEGATIVO, aunque la clase NEUTRO fue la más difícil de predecir (menor recall y f1-score).\n",
    "- **TfidfVectorizer**: Linear SVM también fue el mejor (Accuracy: 0.81, F1 macro: 0.71), pero la clase NEUTRO sigue siendo la menos representada correctamente.\n",
    "En general, **Linear SVM con CountVectorizer** fue el modelo más robusto, logrando el mejor balance entre precisión y F1-score macro. Sin embargo, todos los modelos presentan dificultades para clasificar correctamente la clase NEUTRO, posiblemente por desbalance de clases o menor información en los textos asociados.\n",
    "**Conclusión:** El modelo recomendado es **Linear SVM con CountVectorizer**, ya que ofrece el mejor desempeño global en este problema de clasificación de sentimientos en tweets en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "#print(X_test,y_test)\n",
    "print(X_test.tolist(),y_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fe2d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a67fb4a5",
   "metadata": {},
   "source": [
    "\n",
    "# Empaquetado de Modelos para SageMaker\n",
    "- Verifica que el pipeline elegido (modelo + vectorizador) quede en un archivo único (`model.pkl`/`joblib`), listo para cargar en el handler de `deploy.ipynb`.\n",
    "- Arma `model.tar.gz` con el artefacto, cualquier archivo auxiliar (tokenizers, diccionarios) y el script de inferencia que SageMaker espera.\n",
    "- Ejecuta la celda de registro de dependencias para generar `requirements_sentimientos.txt`; úsalo en la imagen/notebook de despliegue para instalar versiones idénticas a las de entrenamiento.\n",
    "- Sube ambos (`model.tar.gz` y `requirements_sentimientos.txt`) al bucket S3 y referencia el requirements en tu contenedor o paso de instalación para evitar incompatibilidades en CloudWatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd3a36",
   "metadata": {},
   "source": [
    "\n",
    "## Registro de dependencias y freeze (ejecutar después de entrenar)\n",
    "Esta celda captura versiones clave y genera `requirements_sentimientos.txt` con `pip freeze`. Úsalo tanto para volver a entrenar como para el contenedor de inferencia en SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e21ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import json\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def pkg_version(name):\n",
    "    try:\n",
    "        return importlib.import_module(name).__version__\n",
    "    except Exception:\n",
    "        return \"no encontrado\"\n",
    "\n",
    "summary = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"packages\": {\n",
    "        \"pandas\": pkg_version(\"pandas\"),\n",
    "        \"numpy\": pkg_version(\"numpy\"),\n",
    "        \"scikit-learn\": pkg_version(\"sklearn\"),\n",
    "        \"joblib\": pkg_version(\"joblib\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "freeze_output = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "with open(\"requirements_sentimientos.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(freeze_output)\n",
    "\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n",
    "print(\"Archivo requirements_sentimientos.txt generado en el directorio actual.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3de5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_code(src_dir: Path, dest_dir: Path):\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "        shutil.copy(src_dir / fname, dest_dir / fname)\n",
    "\n",
    "def freeze_requirements(output_path: str = \"models/requirements_sentimientos.txt\"):\n",
    "    \"\"\"Genera un pip freeze de la sesión actual para reproducibilidad.\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Generando freeze de dependencias en {output_path}...\")\n",
    "    freeze = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "    output_path.write_text(freeze)\n",
    "    print(\"Freeze generado.\")\n",
    "\n",
    "# Congelar dependencias del entorno actual\n",
    "freeze_requirements()\n",
    "\n",
    "# --- 1. Empaquetar Linear SVM + CountVectorizer ---\n",
    "count_dir = Path('models/svm_countvectorizer')\n",
    "prepare_code(Path('modelos/sentimientos/svm_countvectorizer/code'), count_dir / 'code')\n",
    "output_filename = 'models/model_svm_countvectorizer.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(count_dir / 'model.joblib', arcname='model.joblib')\n",
    "    tar.add(count_dir / 'vectorizer.joblib', arcname='vectorizer.joblib')\n",
    "    for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "        tar.add(count_dir / 'code' / fname, arcname=f'code/{fname}')\n",
    "\n",
    "print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "\n",
    "# --- 2. Empaquetar Linear SVM + TfidfVectorizer ---\n",
    "tfidf_dir = Path('models/svm_tfidfvectorizer')\n",
    "prepare_code(Path('modelos/sentimientos/svm_tfidfvectorizer/code'), tfidf_dir / 'code')\n",
    "output_filename = 'models/model_svm_tfidfvectorizer.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(tfidf_dir / 'model.joblib', arcname='model.joblib')\n",
    "    tar.add(tfidf_dir / 'vectorizer.joblib', arcname='vectorizer.joblib')\n",
    "    for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "        tar.add(tfidf_dir / 'code' / fname, arcname=f'code/{fname}')\n",
    "\n",
    "print(f\"Modelo {output_filename} creado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "\n",
    "#Comparación Completa de Modelos y Análisis Final\n",
    "## Comparación de Rendimiento de Modelos\n",
    "Modelo\tAccuracy\tF1-macro\n",
    "Linear SVM + CountVectorizer\t0.8200\t0.7700\n",
    "Linear SVM + TfidfVectorizer\t0.8100\t0.7100\n",
    "\n",
    "# Conclusiones del Análisis\n",
    "1. Linear SVM con CountVectorizer mantiene el mejor equilibrio entre accuracy (0.82) y F1 macro (0.77).\n",
    "2. El modelo con TfidfVectorizer queda ligeramente por debajo pero sigue siendo competitivo y puede ser útil si se prioriza un vocabulario ponderado.\n",
    "3. La clase NEUTRO es la más retadora; podría beneficiarse de más datos o ajustes de balance de clases.\n",
    "4. Para despliegue se recomienda serializar el pipeline elegido y conservar el vectorizador correspondiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ba446",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a866bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
