{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc5830e",
   "metadata": {},
   "source": [
    "Utilizando el dataset \" Twitter Sentiment Analysis in Spanish Tweets\", deber√°s\n",
    "implementar y comparar diferentes t√©cnicas de Inteligencia Artificial y Modelos de\n",
    "Lenguaje (LLM) para la clasificaci√≥n de sentimientos en comentarios de usuarios.\n",
    "‚Ä¢ Demostrar conocimientos pr√°cticos en t√©cnicas de IA/ML\n",
    "‚Ä¢ Evaluar capacidad de prompt engineering con LLMs\n",
    "‚Ä¢ Analizar cr√≠ticamente los resultados obtenidos\n",
    "‚Ä¢ Muestra a utilizar: Los primeros 100 datos seleccionados aleatoriamente\n",
    "‚Ä¢ Etiquetas esperadas: POSITIVO, NEGATIVO, NEUTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5efa3",
   "metadata": {},
   "source": [
    "\n",
    "## Plan de reentrenamiento reproducible\n",
    "1. Preparar entorno limpio (usa `venv` o `conda`) e instala solo las dependencias necesarias (`pandas`, `numpy`, `scikit-learn`, `pysentimiento`, `joblib`, `tarfile` ya viene con Python). Mant√©n la misma versi√≥n de Python que usar√°s en `deploy.ipynb`.\n",
    "2. Descargar y descomprimir el dataset \"Twitter Sentiment Analysis in Spanish Tweets\". Si vuelves a muestrear, fija la semilla para poder replicar (ej. `random_state=42`).\n",
    "3. Limpieza y preprocesado: normaliza texto, elimina duplicados si aparecen y mapea las etiquetas a `POSITIVO`, `NEGATIVO`, `NEUTRO` como se usa en las m√©tricas.\n",
    "4. Divisi√≥n de datos: separa en train/test (y valid si lo necesitas) manteniendo estratificaci√≥n para no perder la proporci√≥n de clases.\n",
    "5. Entrenamiento comparativo: entrena Logistic Regression, MultinomialNB y Linear SVM con `CountVectorizer` y `TfidfVectorizer`. Guarda las m√©tricas (accuracy, f1-macro y por clase) en tablas reproducibles.\n",
    "6. Interpretaci√≥n: documenta por qu√© eliges el modelo final (actualmente Linear SVM + CountVectorizer). Conserva tambi√©n el `vectorizer` para inferencia.\n",
    "7. Serializaci√≥n: guarda el pipeline completo (`joblib.dump`) y arma `model.tar.gz` con el artefacto y el script de inferencia usado por `deploy.ipynb`.\n",
    "8. Congela las dependencias antes de subir a S3 ejecutando la celda \"Registro de dependencias y freeze\"; copia `requirements_sentimientos.txt` junto al `model.tar.gz` para SageMaker y evita errores de versi√≥n en CloudWatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0047f467",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06cbee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.14.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#%pip install pandas scikit-learn pysentimiento  jupyter ipywidgets\n",
    "\n",
    "#%pip install --upgrade pip\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentiment_analysis_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categor√≠as en 'emotion':\", df['emotion'].unique())\n",
    "print(\"Categor√≠as en 'sentiment':\", df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios de mapeo\n",
    "emotion_map = {\n",
    "    'overwhelmed': 'NEGATIVO', 'embarrassed': 'NEGATIVO', 'jealous': 'NEGATIVO',\n",
    "    'irritated': 'NEGATIVO', 'frustrated': 'NEGATIVO', 'distant': 'NEGATIVO',\n",
    "    'stupid': 'NEGATIVO', 'isolated': 'NEGATIVO', 'sleepy': 'NEGATIVO',\n",
    "\n",
    "    'responsive': 'NEUTRO', 'relaxed': 'NEUTRO',\n",
    "\n",
    "    'loving': 'POSITIVO', 'thankful': 'POSITIVO', 'secure': 'POSITIVO',\n",
    "    'confident': 'POSITIVO', 'successful': 'POSITIVO', 'surprised': 'POSITIVO',\n",
    "    'playful': 'POSITIVO', 'optimistic': 'POSITIVO', 'daring': 'POSITIVO'\n",
    "}\n",
    "\n",
    "sentiment_map = {\n",
    "    'scared': 'NEGATIVO', 'mad': 'NEGATIVO', 'sad': 'NEGATIVO',\n",
    "    'peaceful': 'NEUTRO',\n",
    "    'powerful': 'POSITIVO', 'joyful': 'POSITIVO'\n",
    "}\n",
    "\n",
    "# Combinar ambos mapas para una sola columna final\n",
    "def combine_sentiment(row):\n",
    "    e = emotion_map.get(row['emotion'], 'NEUTRO')\n",
    "    s = sentiment_map.get(row['sentiment'], 'NEUTRO')\n",
    "    # Regla: si alguno es NEGATIVO => NEGATIVO; si alguno es POSITIVO => POSITIVO\n",
    "    if 'NEGATIVO' in (e, s):\n",
    "        return 'NEGATIVO'\n",
    "    elif 'POSITIVO' in (e, s):\n",
    "        return 'POSITIVO'\n",
    "    else:\n",
    "        return 'NEUTRO'\n",
    "\n",
    "df['sentiment_label'] = df.apply(combine_sentiment, axis=1)\n",
    "\n",
    "# Verifica el resultado\n",
    "print(df[['text', 'emotion', 'sentiment', 'sentiment_label']].sample(10))\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "# usar dataset completo\n",
    "\n",
    "df_sample = df.copy()  # usar todo el dataset\n",
    "print(df_sample['sentiment_label'].value_counts())\n",
    "\n",
    "\n",
    "# Selecciona el texto y la etiqueta\n",
    "X = df_sample['text']\n",
    "y = df_sample['sentiment_label']\n",
    "\n",
    "# Divide en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Prueba ambas vectorizaciones\n",
    "vectorizers = {\n",
    "    \"CountVectorizer\": CountVectorizer(),\n",
    "    \"TfidfVectorizer\": TfidfVectorizer()\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Crear directorios para guardar los modelos\n",
    "os.makedirs('models/svm_countvectorizer', exist_ok=True)\n",
    "os.makedirs('models/svm_tfidfvectorizer', exist_ok=True)\n",
    "\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n--- Usando {vec_name} ---\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nModelo: {model_name}\")\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=['POSITIVO', 'NEGATIVO', 'NEUTRO'])\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1-score (macro): {f1:.4f}\")\n",
    "        print(\"Matriz de confusi√≥n:\")\n",
    "        print(cm)\n",
    "        print(\"Reporte de clasificaci√≥n:\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "        # Guardar solo los modelos Linear SVM y sus vectorizadores\n",
    "        if model_name == \"Linear SVM\":\n",
    "            if vec_name == \"CountVectorizer\":\n",
    "                joblib.dump(model, 'models/svm_countvectorizer/model.joblib')\n",
    "                joblib.dump(vectorizer, 'models/svm_countvectorizer/vectorizer.joblib')\n",
    "            elif vec_name == \"TfidfVectorizer\":\n",
    "                joblib.dump(model, 'models/svm_tfidfvectorizer/model.joblib')\n",
    "                joblib.dump(vectorizer, 'models/svm_tfidfvectorizer/vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fccf7",
   "metadata": {},
   "source": [
    "## Interpretaci√≥n de resultados y modelo recomendado\n",
    "Se compararon tres modelos de clasificaci√≥n de sentimientos (Logistic Regression, Naive Bayes y Linear SVM) usando dos t√©cnicas de vectorizaci√≥n de texto (CountVectorizer y TfidfVectorizer).\n",
    "- **CountVectorizer**: Linear SVM obtuvo la mejor precisi√≥n (Accuracy: 0.82, F1 macro: 0.77), mostrando buen desempe√±o en las clases POSITIVO y NEGATIVO, aunque la clase NEUTRO fue la m√°s dif√≠cil de predecir (menor recall y f1-score).\n",
    "- **TfidfVectorizer**: Linear SVM tambi√©n fue el mejor (Accuracy: 0.81, F1 macro: 0.71), pero la clase NEUTRO sigue siendo la menos representada correctamente.\n",
    "En general, **Linear SVM con CountVectorizer** fue el modelo m√°s robusto, logrando el mejor balance entre precisi√≥n y F1-score macro. Sin embargo, todos los modelos presentan dificultades para clasificar correctamente la clase NEUTRO, posiblemente por desbalance de clases o menor informaci√≥n en los textos asociados.\n",
    "**Conclusi√≥n:** El modelo recomendado es **Linear SVM con CountVectorizer**, ya que ofrece el mejor desempe√±o global en este problema de clasificaci√≥n de sentimientos en tweets en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "#print(X_test,y_test)\n",
    "print(X_test.tolist(),y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d088f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"ACCELERATE_USE_CPU\"] = \"true\"\n",
    "from pysentimiento import create_analyzer\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\", device=\"cpu\")\n",
    "resultado = analyzer.predict(X_test.iloc[0])\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para mapear las etiquetas de pysentimiento a nuestro formato\n",
    "def map_pysentimiento_label(pysentimiento_label):\n",
    "    \"\"\"\n",
    "    Mapea las etiquetas de pysentimiento a nuestro formato\n",
    "    NEU -> NEUTRO\n",
    "    POS -> POSITIVO  \n",
    "    NEG -> NEGATIVO\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'NEU': 'NEUTRO',\n",
    "        'POS': 'POSITIVO', \n",
    "        'NEG': 'NEGATIVO'\n",
    "    }\n",
    "    return mapping.get(pysentimiento_label, 'NEUTRO')\n",
    "\n",
    "# Funci√≥n para formatear texto largo con saltos de l√≠nea\n",
    "def format_long_text(text, max_width=80):\n",
    "    \"\"\"Divide el texto en l√≠neas de m√°ximo max_width caracteres\"\"\"\n",
    "    import textwrap\n",
    "    return '\\n'.join(textwrap.wrap(text, width=max_width))\n",
    "\n",
    "# Ejemplo con el resultado actual - Versi√≥n mejorada para mejor visualizaci√≥n\n",
    "print(\"=\" * 60)\n",
    "print(\"AN√ÅLISIS DE MUESTRA DE PRUEBA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìù TEXTO DE PRUEBA:\")\n",
    "print(\"-\" * 40)\n",
    "print(format_long_text(X_test.iloc[0]))\n",
    "\n",
    "print(f\"\\nüéØ ETIQUETA REAL: {y_test.iloc[0]}\")\n",
    "\n",
    "print(f\"\\nü§ñ PREDICCI√ìN PYSENTIMIENTO:\")\n",
    "print(f\"   ‚Ä¢ Etiqueta original: {resultado.output}\")\n",
    "print(f\"   ‚Ä¢ Etiqueta mapeada: {map_pysentimiento_label(resultado.output)}\")\n",
    "\n",
    "print(f\"\\nüìä PROBABILIDADES:\")\n",
    "for label, prob in resultado.probas.items():\n",
    "    mapped_label = map_pysentimiento_label(label)\n",
    "    print(f\"   ‚Ä¢ {mapped_label}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fe2d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b21554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todas las muestras de prueba con pysentimiento\n",
    "print(\"Procesando\", len(X_test), \"muestras de prueba...\")\n",
    "print(\"Esto puede tomar unos momentos...\")\n",
    "\n",
    "y_pred_pysentimiento = []\n",
    "probabilidades_pysentimiento = []\n",
    "\n",
    "for i, texto in enumerate(X_test):\n",
    "    print(f\"Procesando muestra {i+1}/{len(X_test)}\", end='\\r')\n",
    "    \n",
    "    # Hacer predicci√≥n\n",
    "    resultado = analyzer.predict(texto)\n",
    "    \n",
    "    # Extraer etiqueta predicha y mapearla\n",
    "    etiqueta_predicha = map_pysentimiento_label(resultado.output)\n",
    "    y_pred_pysentimiento.append(etiqueta_predicha)\n",
    "    \n",
    "    # Guardar probabilidades (opcional, para an√°lisis posterior)\n",
    "    probabilidades_pysentimiento.append(resultado.probas)\n",
    "\n",
    "print(\"\\n¬°Predicciones completadas!\")\n",
    "print(f\"Total de predicciones: {len(y_pred_pysentimiento)}\")\n",
    "print(f\"Primeras 5 predicciones: {y_pred_pysentimiento[:5]}\")\n",
    "print(f\"Primeras 5 etiquetas reales: {y_test.tolist()[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n del modelo pysentimiento\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Convertir y_test a lista para la comparaci√≥n\n",
    "y_test_list = y_test.tolist()\n",
    "\n",
    "print(\"=== EVALUACI√ìN DEL MODELO PYSENTIMIENTO ===\")\n",
    "print(f\"Muestras evaluadas: {len(y_test_list)}\")\n",
    "print()\n",
    "\n",
    "# M√©tricas principales\n",
    "accuracy = accuracy_score(y_test_list, y_pred_pysentimiento)\n",
    "f1_macro = f1_score(y_test_list, y_pred_pysentimiento, average='macro')\n",
    "f1_weighted = f1_score(y_test_list, y_pred_pysentimiento, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (Precisi√≥n): {accuracy:.4f}\")\n",
    "print(f\"F1-score (macro): {f1_macro:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_weighted:.4f}\")\n",
    "print()\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "labels = ['POSITIVO', 'NEGATIVO', 'NEUTRO']\n",
    "cm = confusion_matrix(y_test_list, y_pred_pysentimiento, labels=labels)\n",
    "print(\"Matriz de Confusi√≥n:\")\n",
    "print(\"Filas: Etiquetas reales, Columnas: Predicciones\")\n",
    "print(\"                 POSITIVO  NEGATIVO  NEUTRO\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label:>12} {cm[i][0]:>8} {cm[i][1]:>8} {cm[i][2]:>8}\")\n",
    "print()\n",
    "\n",
    "# Reporte detallado de clasificaci√≥n\n",
    "print(\"Reporte de Clasificaci√≥n Detallado:\")\n",
    "print(classification_report(y_test_list, y_pred_pysentimiento, labels=labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n con modelos anteriores\n",
    "print(\"=== COMPARACI√ìN DE MODELOS ===\")\n",
    "print()\n",
    "\n",
    "# Resultados de modelos anteriores (de tu an√°lisis previo)\n",
    "resultados_modelos = {\n",
    "    \"Linear SVM + CountVectorizer\": {\"accuracy\": 0.82, \"f1_macro\": 0.77},\n",
    "    \"Linear SVM + TfidfVectorizer\": {\"accuracy\": 0.81, \"f1_macro\": 0.71},\n",
    "    \"PysentimientoLLM\": {\"accuracy\": accuracy, \"f1_macro\": f1_macro}\n",
    "}\n",
    "\n",
    "print(\"Modelo                      Accuracy  F1-macro\")\n",
    "print(\"-\" * 50)\n",
    "for modelo, metricas in resultados_modelos.items():\n",
    "    print(f\"{modelo:<25} {metricas['accuracy']:>8.4f} {metricas['f1_macro']:>8.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=== CONCLUSIONES ===\")\n",
    "print(\"1. PysentimientoLLM obtuvo una accuracy de {:.1f}% y F1-macro de {:.1f}%\".format(accuracy*100, f1_macro*100))\n",
    "print(\"2. Los modelos tradicionales (SVM) superaron significativamente al LLM en esta muestra\")\n",
    "print(\"3. El modelo PysentimientoLLM tuvo mejor desempe√±o en NEGATIVO que en POSITIVO y NEUTRO\")\n",
    "print(\"4. La clase NEUTRO sigue siendo la m√°s dif√≠cil de clasificar para todos los modelos\")\n",
    "\n",
    "# An√°lisis de probabilidades (opcional)\n",
    "print()\n",
    "print(\"=== AN√ÅLISIS DE CONFIANZA (PROBABILIDADES) ===\")\n",
    "confianzas = []\n",
    "for probs in probabilidades_pysentimiento:\n",
    "    max_prob = max(probs.values())\n",
    "    confianzas.append(max_prob)\n",
    "\n",
    "print(f\"Confianza promedio: {np.mean(confianzas):.3f}\")\n",
    "print(f\"Confianza m√≠nima: {np.min(confianzas):.3f}\")\n",
    "print(f\"Confianza m√°xima: {np.max(confianzas):.3f}\")\n",
    "print(f\"Predicciones con alta confianza (>0.7): {sum(1 for c in confianzas if c > 0.7)}/{len(confianzas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fb4a5",
   "metadata": {},
   "source": [
    "\n",
    "# Empaquetado de Modelos para SageMaker\n",
    "- Verifica que el pipeline elegido (modelo + vectorizador) quede en un archivo √∫nico (`model.pkl`/`joblib`), listo para cargar en el handler de `deploy.ipynb`.\n",
    "- Arma `model.tar.gz` con el artefacto, cualquier archivo auxiliar (tokenizers, diccionarios) y el script de inferencia que SageMaker espera.\n",
    "- Ejecuta la celda de registro de dependencias para generar `requirements_sentimientos.txt`; √∫salo en la imagen/notebook de despliegue para instalar versiones id√©nticas a las de entrenamiento.\n",
    "- Sube ambos (`model.tar.gz` y `requirements_sentimientos.txt`) al bucket S3 y referencia el requirements en tu contenedor o paso de instalaci√≥n para evitar incompatibilidades en CloudWatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd3a36",
   "metadata": {},
   "source": [
    "\n",
    "## Registro de dependencias y freeze (ejecutar despu√©s de entrenar)\n",
    "Esta celda captura versiones clave y genera `requirements_sentimientos.txt` con `pip freeze`. √ösalo tanto para volver a entrenar como para el contenedor de inferencia en SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import json\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def pkg_version(name):\n",
    "    try:\n",
    "        return importlib.import_module(name).__version__\n",
    "    except Exception:\n",
    "        return \"no encontrado\"\n",
    "\n",
    "summary = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"packages\": {\n",
    "        \"pandas\": pkg_version(\"pandas\"),\n",
    "        \"numpy\": pkg_version(\"numpy\"),\n",
    "        \"scikit-learn\": pkg_version(\"sklearn\"),\n",
    "        \"pysentimiento\": pkg_version(\"pysentimiento\"),\n",
    "        \"joblib\": pkg_version(\"joblib\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "freeze_output = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "with open(\"requirements_sentimientos.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(freeze_output)\n",
    "\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n",
    "print(\"Archivo requirements_sentimientos.txt generado en el directorio actual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e21ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_code(src_dir: Path, dest_dir: Path):\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "        shutil.copy(src_dir / fname, dest_dir / fname)\n",
    "\n",
    "def freeze_requirements(output_path: str = \"models/requirements_sentimientos.txt\"):\n",
    "    \"\"\"Genera un pip freeze de la sesi√≥n actual para reproducibilidad.\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Generando freeze de dependencias en {output_path}...\")\n",
    "    freeze = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "    output_path.write_text(freeze)\n",
    "    print(\"Freeze generado.\")\n",
    "\n",
    "# Congelar dependencias del entorno actual\n",
    "freeze_requirements()\n",
    "\n",
    "# --- 1. Empaquetar Linear SVM + CountVectorizer ---\n",
    "count_dir = Path('models/svm_countvectorizer')\n",
    "prepare_code(Path('modelos/sentimientos/svm_countvectorizer/code'), count_dir / 'code')\n",
    "output_filename = 'models/model_svm_countvectorizer.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(count_dir / 'model.joblib', arcname='model.joblib')\n",
    "    tar.add(count_dir / 'vectorizer.joblib', arcname='vectorizer.joblib')\n",
    "    for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "        tar.add(count_dir / 'code' / fname, arcname=f'code/{fname}')\n",
    "\n",
    "print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "\n",
    "# --- 2. Empaquetar Linear SVM + TfidfVectorizer ---\n",
    "tfidf_dir = Path('models/svm_tfidfvectorizer')\n",
    "prepare_code(Path('modelos/sentimientos/svm_tfidfvectorizer/code'), tfidf_dir / 'code')\n",
    "output_filename = 'models/model_svm_tfidfvectorizer.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(tfidf_dir / 'model.joblib', arcname='model.joblib')\n",
    "    tar.add(tfidf_dir / 'vectorizer.joblib', arcname='vectorizer.joblib')\n",
    "    for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "        tar.add(tfidf_dir / 'code' / fname, arcname=f'code/{fname}')\n",
    "\n",
    "print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "\n",
    "# --- 3. Empaquetar PysentimientoLLM ---\n",
    "# Pysentimiento descarga los modelos en un cach√©. Intentamos usar una copia local\n",
    "# y, si no existe, descargamos y empaquetamos desde el cach√© de HuggingFace.\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "print(\"Preparando artefacto de pysentimiento...\")\n",
    "local_model_dir = Path('modelos/sentimientos/model_pysentimiento')\n",
    "hf_cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'\n",
    "model_name = 'pysentimiento/robertuito-sentiment-analysis'\n",
    "\n",
    "# Preferir copia local (incluida en el repo)\n",
    "model_source_path = None\n",
    "if (local_model_dir / 'config.json').exists() and any(local_model_dir.glob('*.safetensors')):\n",
    "    model_source_path = local_model_dir\n",
    "else:\n",
    "    for path in hf_cache_dir.glob(f\"models--{model_name.replace('/', '--')}/snapshots/*\"):\n",
    "        if path.is_dir():\n",
    "            model_source_path = path\n",
    "            break\n",
    "    if model_source_path is None:\n",
    "        # Fuerza la descarga si no estaba en cach√©\n",
    "        analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "        for path in hf_cache_dir.glob(f\"models--{model_name.replace('/', '--')}/snapshots/*\"):\n",
    "            if path.is_dir():\n",
    "                model_source_path = path\n",
    "                break\n",
    "\n",
    "# Preparar c√≥digo de inferencia para pysentimiento\n",
    "pysent_dir = Path('models/model_pysentimiento')\n",
    "prepare_code(Path('modelos/sentimientos/model_pysentimiento/code'), pysent_dir / 'code')\n",
    "\n",
    "output_filename = 'models/model_pysentimiento.tar.gz'\n",
    "if model_source_path:\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        print(f\"Empaquetando modelo desde {model_source_path}\")\n",
    "        tar.add(model_source_path, arcname='model')\n",
    "        for fname in [\"inference.py\", \"__init__.py\", \"requirements.txt\"]:\n",
    "            tar.add(pysent_dir / 'code' / fname, arcname=f'code/{fname}')\n",
    "    print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No se encontr√≥ el modelo de pysentimiento ni en local ni en cach√©. Ejecuta create_analyzer antes de empaquetar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3de5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "#Comparaci√≥n Completa de Modelos y An√°lisis Final\n",
    "## Comparaci√≥n de Rendimiento de Modelos\n",
    "Modelo\tAccuracy\tF1-macro\n",
    "Linear SVM + CountVectorizer\t0.8200\t0.7700\n",
    "Linear SVM + TfidfVectorizer\t0.8100\t0.7100\n",
    "PysentimientoLLM\t0.4500\t0.4045\n",
    "An√°lisis de Confianza (PysentimientoLLM)\n",
    "Confianza promedio: 0.745\n",
    "Confianza m√≠nima: 0.454\n",
    "Confianza m√°xima: 0.981\n",
    "Predicciones con alta confianza (>0.7): 12/20\n",
    "# Conclusiones del An√°lisis\n",
    "###1. Rendimiento General de los Modelos\n",
    "PysentimientoLLM obtuvo una accuracy de 45.0% y F1-macro de 40.4%, lo que representa un rendimiento considerablemente inferior comparado con los modelos tradicionales. Los modelos tradicionales (SVM) superaron significativamente al LLM en esta muestra, con Linear SVM + CountVectorizer alcanzando 82% de accuracy versus 45% del modelo de lenguaje, una diferencia de 37 puntos porcentuales.\n",
    "\n",
    "### 2. An√°lisis por Clase de Sentimiento\n",
    "El modelo PysentimientoLLM tuvo mejor desempe√±o en NEGATIVO que en POSITIVO y NEUTRO, con los siguientes resultados por clase:\n",
    "\n",
    "NEGATIVO: F1-score = 0.6316 (mejor desempe√±o)\n",
    "NEUTRO: F1-score = 0.4000 (desempe√±o intermedio)\n",
    "POSITIVO: F1-score = 0.1818 (mayor dificultad)\n",
    "Contrario a lo observado en los modelos SVM donde NEUTRO era la clase m√°s problem√°tica, para PysentimientoLLM la clase POSITIVO es la m√°s dif√≠cil de clasificar, no NEUTRO. Esto sugiere que el modelo pre-entrenado tiene sesgos espec√≠ficos hacia la detecci√≥n de sentimientos negativos.\n",
    "\n",
    "### 3. Problema de Overconfidence\n",
    "Un hallazgo cr√≠tico es que el modelo PysentimientoLLM muestra alta confianza promedio (74.5%) pero baja precisi√≥n (45%). Esto indica un problema de \"overconfidence\": el modelo est√° muy seguro de predicciones que resultan ser incorrectas. De las 20 muestras evaluadas, 12 tuvieron alta confianza (>0.7), pero solo 9 fueron clasificadas correctamente.\n",
    "\n",
    "### 4. Ranking de Dificultad por Clase\n",
    "Para el modelo PysentimientoLLM, el ranking de dificultad es:\n",
    "\n",
    "POSITIVO: F1-score = 0.1818 (M√ÅS DIF√çCIL)\n",
    "NEUTRO: F1-score = 0.4000 (INTERMEDIO)\n",
    "NEGATIVO: F1-score = 0.6316 (M√ÅS F√ÅCIL)\n",
    "Recomendaciones y Consideraciones\n",
    "Modelo Recomendado\n",
    "Linear SVM con CountVectorizer es el modelo recomendado para este dataset espec√≠fico, ofreciendo el mejor balance entre accuracy (82%) y F1-score macro (77%).\n",
    "\n",
    "#### Consideraciones sobre LLMs\n",
    "Aunque PysentimientoLLM es un modelo pre-entrenado sofisticado, su rendimiento inferior en este caso espec√≠fico sugiere que:\n",
    "\n",
    "**Los modelos tradicionales pueden ser m√°s efectivos para datasets peque√±os** y bien estructurados\n",
    "El fine-tuning espec√≠fico del dominio podr√≠a ser necesario para mejorar el rendimiento del LLM\n",
    "La eficiencia computacional de los modelos SVM es superior para esta tarea\n",
    "El prompt engineering m√°s sofisticado podr√≠a mejorar los resultados del modelo de lenguaje\n",
    "Implicaciones Pr√°cticas\n",
    "Para implementaciones en producci√≥n con este tipo de datos en espa√±ol, los modelos tradicionales de machine learning siguen siendo una opci√≥n robusta y eficiente, especialmente cuando se cuenta con datasets etiquetados de calidad y recursos computacionales limitados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ba446",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a866bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
