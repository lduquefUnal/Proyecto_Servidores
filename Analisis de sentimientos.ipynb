{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc5830e",
   "metadata": {},
   "source": [
    "Utilizando el dataset \" Twitter Sentiment Analysis in Spanish Tweets\", deber√°s\n",
    "implementar y comparar diferentes t√©cnicas de Inteligencia Artificial y Modelos de\n",
    "Lenguaje (LLM) para la clasificaci√≥n de sentimientos en comentarios de usuarios.\n",
    "‚Ä¢ Demostrar conocimientos pr√°cticos en t√©cnicas de IA/ML\n",
    "‚Ä¢ Evaluar capacidad de prompt engineering con LLMs\n",
    "‚Ä¢ Analizar cr√≠ticamente los resultados obtenidos\n",
    "‚Ä¢ Muestra a utilizar: Los primeros 100 datos seleccionados aleatoriamente\n",
    "‚Ä¢ Etiquetas esperadas: POSITIVO, NEGATIVO, NEUTRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af06cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas scikit-learn pysentimiento  jupyter ipywidgets\n",
    "\n",
    "#%pip install --upgrade pip\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad8e859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "user",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emotion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b19f6be9-e5e5-4d74-a293-d8d0282a2d1c",
       "rows": [
        [
         "0",
         "@erreborda",
         "termine bien abrumado despu√©s de hoy",
         "Jan 6, 2024 ¬∑ 2:53 AM UTC",
         "overwhelmed",
         "scared"
        ],
        [
         "1",
         "@shpiderduck",
         "me siento abrumado",
         "Jan 6, 2024 ¬∑ 2:35 AM UTC",
         "overwhelmed",
         "scared"
        ],
        [
         "2",
         "@Alex_R_art",
         "Me siento un poco abrumado por la cantidad de cosas que quiero dibujar, ver, jugar y leer. Odio esta sensaci√≥n xdddd",
         "Jan 6, 2024 ¬∑ 12:20 AM UTC",
         "overwhelmed",
         "scared"
        ],
        [
         "3",
         "@anggelinaa97",
         "Salvador la √∫nica persona que no la ha abrumado de versiones‚ù§üòí‚ù§ #NadieComoT√∫",
         "Jan 5, 2024 ¬∑ 10:38 PM UTC",
         "overwhelmed",
         "scared"
        ],
        [
         "4",
         "@diegoreyesvqz",
         "Denme un helado o algo que ando full abrumado.",
         "Jan 5, 2024 ¬∑ 8:38 PM UTC",
         "overwhelmed",
         "scared"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>emotion</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@erreborda</td>\n",
       "      <td>termine bien abrumado despu√©s de hoy</td>\n",
       "      <td>Jan 6, 2024 ¬∑ 2:53 AM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@shpiderduck</td>\n",
       "      <td>me siento abrumado</td>\n",
       "      <td>Jan 6, 2024 ¬∑ 2:35 AM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Alex_R_art</td>\n",
       "      <td>Me siento un poco abrumado por la cantidad de ...</td>\n",
       "      <td>Jan 6, 2024 ¬∑ 12:20 AM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anggelinaa97</td>\n",
       "      <td>Salvador la √∫nica persona que no la ha abrumad...</td>\n",
       "      <td>Jan 5, 2024 ¬∑ 10:38 PM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@diegoreyesvqz</td>\n",
       "      <td>Denme un helado o algo que ando full abrumado.</td>\n",
       "      <td>Jan 5, 2024 ¬∑ 8:38 PM UTC</td>\n",
       "      <td>overwhelmed</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user                                               text  \\\n",
       "0      @erreborda               termine bien abrumado despu√©s de hoy   \n",
       "1    @shpiderduck                                 me siento abrumado   \n",
       "2     @Alex_R_art  Me siento un poco abrumado por la cantidad de ...   \n",
       "3   @anggelinaa97  Salvador la √∫nica persona que no la ha abrumad...   \n",
       "4  @diegoreyesvqz     Denme un helado o algo que ando full abrumado.   \n",
       "\n",
       "                         date      emotion sentiment  \n",
       "0   Jan 6, 2024 ¬∑ 2:53 AM UTC  overwhelmed    scared  \n",
       "1   Jan 6, 2024 ¬∑ 2:35 AM UTC  overwhelmed    scared  \n",
       "2  Jan 6, 2024 ¬∑ 12:20 AM UTC  overwhelmed    scared  \n",
       "3  Jan 5, 2024 ¬∑ 10:38 PM UTC  overwhelmed    scared  \n",
       "4   Jan 5, 2024 ¬∑ 8:38 PM UTC  overwhelmed    scared  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentiment_analysis_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1fd8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categor√≠as en 'emotion': ['overwhelmed' 'embarrassed' 'jealous' 'irritated' 'frustrated' 'distant'\n",
      " 'stupid' 'isolated' 'sleepy' 'responsive' 'relaxed' 'loving' 'thankful'\n",
      " 'secure' 'confident' 'successful' 'surprised' 'playful' 'optimistic'\n",
      " 'daring']\n",
      "Categor√≠as en 'sentiment': ['scared' 'mad' 'sad' 'peaceful' 'powerful' 'joyful']\n"
     ]
    }
   ],
   "source": [
    "print(\"Categor√≠as en 'emotion':\", df['emotion'].unique())\n",
    "print(\"Categor√≠as en 'sentiment':\", df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e4baa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text      emotion  \\\n",
      "393   amigo q hueso que estoy en el bass, toy frustrado      jealous   \n",
      "2510  \"Un padre y una madre unidos en matrimonio, to...       daring   \n",
      "2345    Eh wey las direccionales no gastan gasolina!!!!      playful   \n",
      "1305  \"De manera apacible, se puede sacudir el mundo...      relaxed   \n",
      "1627  agradecido siempre por quien est√° y por quien ...     thankful   \n",
      "382                          Odio sentir que molesto :c      jealous   \n",
      "300         Alan est√° m√°s despechado lpm   #GranHermano      jealous   \n",
      "1049  \"La arena del desierto es para el viajero fati...       sleepy   \n",
      "911   Cada dia que pasa mas qsco me d ala gente, voy...     isolated   \n",
      "91    ... que Los Reyes Magos, traigan \"lo mejor\", p...  overwhelmed   \n",
      "\n",
      "     sentiment sentiment_label  \n",
      "393        mad        NEGATIVO  \n",
      "2510    joyful        POSITIVO  \n",
      "2345    joyful        POSITIVO  \n",
      "1305  peaceful          NEUTRO  \n",
      "1627  peaceful        POSITIVO  \n",
      "382        mad        NEGATIVO  \n",
      "300        mad        NEGATIVO  \n",
      "1049       sad        NEGATIVO  \n",
      "911        sad        NEGATIVO  \n",
      "91      scared        NEGATIVO  \n",
      "sentiment_label\n",
      "POSITIVO    1190\n",
      "NEGATIVO    1160\n",
      "NEUTRO       240\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Diccionarios de mapeo\n",
    "emotion_map = {\n",
    "    'overwhelmed': 'NEGATIVO', 'embarrassed': 'NEGATIVO', 'jealous': 'NEGATIVO',\n",
    "    'irritated': 'NEGATIVO', 'frustrated': 'NEGATIVO', 'distant': 'NEGATIVO',\n",
    "    'stupid': 'NEGATIVO', 'isolated': 'NEGATIVO', 'sleepy': 'NEGATIVO',\n",
    "\n",
    "    'responsive': 'NEUTRO', 'relaxed': 'NEUTRO',\n",
    "\n",
    "    'loving': 'POSITIVO', 'thankful': 'POSITIVO', 'secure': 'POSITIVO',\n",
    "    'confident': 'POSITIVO', 'successful': 'POSITIVO', 'surprised': 'POSITIVO',\n",
    "    'playful': 'POSITIVO', 'optimistic': 'POSITIVO', 'daring': 'POSITIVO'\n",
    "}\n",
    "\n",
    "sentiment_map = {\n",
    "    'scared': 'NEGATIVO', 'mad': 'NEGATIVO', 'sad': 'NEGATIVO',\n",
    "    'peaceful': 'NEUTRO',\n",
    "    'powerful': 'POSITIVO', 'joyful': 'POSITIVO'\n",
    "}\n",
    "\n",
    "# Combinar ambos mapas para una sola columna final\n",
    "def combine_sentiment(row):\n",
    "    e = emotion_map.get(row['emotion'], 'NEUTRO')\n",
    "    s = sentiment_map.get(row['sentiment'], 'NEUTRO')\n",
    "    # Regla: si alguno es NEGATIVO => NEGATIVO; si alguno es POSITIVO => POSITIVO\n",
    "    if 'NEGATIVO' in (e, s):\n",
    "        return 'NEGATIVO'\n",
    "    elif 'POSITIVO' in (e, s):\n",
    "        return 'POSITIVO'\n",
    "    else:\n",
    "        return 'NEUTRO'\n",
    "\n",
    "df['sentiment_label'] = df.apply(combine_sentiment, axis=1)\n",
    "\n",
    "# Verifica el resultado\n",
    "print(df[['text', 'emotion', 'sentiment', 'sentiment_label']].sample(10))\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aadfb612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_label\n",
      "NEGATIVO    45\n",
      "POSITIVO    45\n",
      "NEUTRO      10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Usando CountVectorizer ---\n",
      "\n",
      "Modelo: Logistic Regression\n",
      "Accuracy: 0.4500\n",
      "F1-score (macro): 0.3068\n",
      "Matriz de confusi√≥n:\n",
      "[[6 3 0]\n",
      " [6 3 0]\n",
      " [1 1 0]]\n",
      "Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVO     0.4286    0.3333    0.3750         9\n",
      "      NEUTRO     0.0000    0.0000    0.0000         2\n",
      "    POSITIVO     0.4615    0.6667    0.5455         9\n",
      "\n",
      "    accuracy                         0.4500        20\n",
      "   macro avg     0.2967    0.3333    0.3068        20\n",
      "weighted avg     0.4005    0.4500    0.4142        20\n",
      "\n",
      "\n",
      "Modelo: Naive Bayes\n",
      "Accuracy: 0.4500\n",
      "F1-score (macro): 0.3148\n",
      "Matriz de confusi√≥n:\n",
      "[[5 4 0]\n",
      " [5 4 0]\n",
      " [1 1 0]]\n",
      "Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVO     0.4444    0.4444    0.4444         9\n",
      "      NEUTRO     0.0000    0.0000    0.0000         2\n",
      "    POSITIVO     0.4545    0.5556    0.5000         9\n",
      "\n",
      "    accuracy                         0.4500        20\n",
      "   macro avg     0.2997    0.3333    0.3148        20\n",
      "weighted avg     0.4045    0.4500    0.4250        20\n",
      "\n",
      "\n",
      "Modelo: Linear SVM\n",
      "Accuracy: 0.5000\n",
      "F1-score (macro): 0.5374\n",
      "Matriz de confusi√≥n:\n",
      "[[6 3 0]\n",
      " [6 3 0]\n",
      " [1 0 1]]\n",
      "Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVO     0.5000    0.3333    0.4000         9\n",
      "      NEUTRO     1.0000    0.5000    0.6667         2\n",
      "    POSITIVO     0.4615    0.6667    0.5455         9\n",
      "\n",
      "    accuracy                         0.5000        20\n",
      "   macro avg     0.6538    0.5000    0.5374        20\n",
      "weighted avg     0.5327    0.5000    0.4921        20\n",
      "\n",
      "\n",
      "--- Usando TfidfVectorizer ---\n",
      "\n",
      "Modelo: Logistic Regression\n",
      "Accuracy: 0.5000\n",
      "F1-score (macro): 0.3473\n",
      "Matriz de confusi√≥n:\n",
      "[[6 3 0]\n",
      " [5 4 0]\n",
      " [1 1 0]]\n",
      "Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVO     0.5000    0.4444    0.4706         9\n",
      "      NEUTRO     0.0000    0.0000    0.0000         2\n",
      "    POSITIVO     0.5000    0.6667    0.5714         9\n",
      "\n",
      "    accuracy                         0.5000        20\n",
      "   macro avg     0.3333    0.3704    0.3473        20\n",
      "weighted avg     0.4500    0.5000    0.4689        20\n",
      "\n",
      "\n",
      "Modelo: Naive Bayes\n",
      "Accuracy: 0.4500\n",
      "F1-score (macro): 0.3148\n",
      "Matriz de confusi√≥n:\n",
      "[[5 4 0]\n",
      " [5 4 0]\n",
      " [1 1 0]]\n",
      "Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVO     0.4444    0.4444    0.4444         9\n",
      "      NEUTRO     0.0000    0.0000    0.0000         2\n",
      "    POSITIVO     0.4545    0.5556    0.5000         9\n",
      "\n",
      "    accuracy                         0.4500        20\n",
      "   macro avg     0.2997    0.3333    0.3148        20\n",
      "weighted avg     0.4045    0.4500    0.4250        20\n",
      "\n",
      "\n",
      "Modelo: Linear SVM\n",
      "Accuracy: 0.5000\n",
      "F1-score (macro): 0.3473\n",
      "Matriz de confusi√≥n:\n",
      "[[6 3 0]\n",
      " [5 4 0]\n",
      " [1 1 0]]\n",
      "Reporte de clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVO     0.5000    0.4444    0.4706         9\n",
      "      NEUTRO     0.0000    0.0000    0.0000         2\n",
      "    POSITIVO     0.5000    0.6667    0.5714         9\n",
      "\n",
      "    accuracy                         0.5000        20\n",
      "   macro avg     0.3333    0.3704    0.3473        20\n",
      "weighted avg     0.4500    0.5000    0.4689        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/luisduquef/Documentos/Universidad/servidores/Proyecto_Servidores/.venv/lib64/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "# selecciono 100 al azar para acelerar pruebas\n",
    "\n",
    "df_sample = df.sample(n=100, random_state=35).reset_index(drop=True)\n",
    "print(df_sample['sentiment_label'].value_counts())\n",
    "\n",
    "\n",
    "# Selecciona el texto y la etiqueta\n",
    "X = df_sample['text']\n",
    "y = df_sample['sentiment_label']\n",
    "\n",
    "# Divide en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Prueba ambas vectorizaciones\n",
    "vectorizers = {\n",
    "    \"CountVectorizer\": CountVectorizer(),\n",
    "    \"TfidfVectorizer\": TfidfVectorizer()\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "# Crear directorios para guardar los modelos\n",
    "os.makedirs('models/svm_countvectorizer', exist_ok=True)\n",
    "os.makedirs('models/svm_tfidfvectorizer', exist_ok=True)\n",
    "\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n--- Usando {vec_name} ---\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nModelo: {model_name}\")\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=['POSITIVO', 'NEGATIVO', 'NEUTRO'])\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"F1-score (macro): {f1:.4f}\")\n",
    "        print(\"Matriz de confusi√≥n:\")\n",
    "        print(cm)\n",
    "        print(\"Reporte de clasificaci√≥n:\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "        # Guardar solo los modelos Linear SVM y sus vectorizadores\n",
    "        if model_name == \"Linear SVM\":\n",
    "            if vec_name == \"CountVectorizer\":\n",
    "                joblib.dump(model, 'models/svm_countvectorizer/model.joblib')\n",
    "                joblib.dump(vectorizer, 'models/svm_countvectorizer/vectorizer.joblib')\n",
    "            elif vec_name == \"TfidfVectorizer\":\n",
    "                joblib.dump(model, 'models/svm_tfidfvectorizer/model.joblib')\n",
    "                joblib.dump(vectorizer, 'models/svm_tfidfvectorizer/vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fccf7",
   "metadata": {},
   "source": [
    "## Interpretaci√≥n de resultados y modelo recomendado\n",
    "Se compararon tres modelos de clasificaci√≥n de sentimientos (Logistic Regression, Naive Bayes y Linear SVM) usando dos t√©cnicas de vectorizaci√≥n de texto (CountVectorizer y TfidfVectorizer).\n",
    "- **CountVectorizer**: Linear SVM obtuvo la mejor precisi√≥n (Accuracy: 0.82, F1 macro: 0.77), mostrando buen desempe√±o en las clases POSITIVO y NEGATIVO, aunque la clase NEUTRO fue la m√°s dif√≠cil de predecir (menor recall y f1-score).\n",
    "- **TfidfVectorizer**: Linear SVM tambi√©n fue el mejor (Accuracy: 0.81, F1 macro: 0.71), pero la clase NEUTRO sigue siendo la menos representada correctamente.\n",
    "En general, **Linear SVM con CountVectorizer** fue el modelo m√°s robusto, logrando el mejor balance entre precisi√≥n y F1-score macro. Sin embargo, todos los modelos presentan dificultades para clasificar correctamente la clase NEUTRO, posiblemente por desbalance de clases o menor informaci√≥n en los textos asociados.\n",
    "**Conclusi√≥n:** El modelo recomendado es **Linear SVM con CountVectorizer**, ya que ofrece el mejor desempe√±o global en este problema de clasificaci√≥n de sentimientos en tweets en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b572652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "['Siempre me sensibiliz√≥ Terminator II cuando le hace üëçüèº a John, y una mente brillante, cuando le dejan las lapiceras en la mesa. Solo eso pasaba a escribir en mi tuiter personal', 'Lo unico que quiero ahora mas que nunca, tener un trabajo estable y irme a vivir solaüôåüòî', 'Independientemente de las falencias estructurales expuestas en esta Nochebuena, quiero expresar mi agradecimiento a los hombres y mujeres de ETESA e IDAAN que TRABAJARON hoy para atender, coordinar, y resolver tan r√°pido. Muchas gracias compatriotas por su servicio.', 'Mucha paz en mi vida. Aunque llegar a este punto no fue nada f√°cil, pero lograrlo es lo m√°s satisfactorio. Ando como muy optimista, sin darle importancia a cosas que antes me romp√≠an la cabeza. Ay no, ya soy otra.', 'quien fue el envidioso ke me tiro mal de ojo para que me salieran mil granos', 'Carla es mufa nunca nos tuvo fe y ahora morimos, matemoslo por viejo amargado', 'Porque lo √∫nico que hace Scoot es subir el bal√≥n y pasarlo a un compa√±ero? Porque nunca penetra o juegan para el? Que pasa ah√≠ , no entiendo nada y me pone muy nervioso @BlazersArgento', 'Nunca hab√≠a estado tan segura de algo, pero puedo decir con toda firmeza que ya no m√°s, √∫ltima vez ü§û', 'Lo fatigado que tengo los brazos quiz√°s esos 8mil litros vinieron con delay', \"Le molest√≥ que escuche miranda cus I'm healing üíã\", 'peter se hizo lobo solitario o que?', 'Hol bebe, que grato es volver a coincidir contigoüòçüé∂', 'En la √∫ltima pel√≠cula de r√°pido y furioso encontraron las esferas del drag√≥n para revivir a todo el mundo o que verga', 'soy un chico tranquilito sosegado calmado adem√°s puro casto y recatado', 'Dia 1: HERMOSO Muy bello el mar con las olas y animalitos, de noche es mas hermoso. El momento de juego de mesa fue muy divertido y para repetir, aunque hubo uno que otro susto y habia algo de sue√±o el dia fue  hermoso en su totalidad 9.5/10', 'Y nuevamente el fr√≠o hace su entrada triunfante en m√≠.', 'Esquizofrenia = mente dividida: Miedo a las realidades paralelas que se le presentan. Sin miedo deben aceptar el llamado de dios y abrir las puertas al cielo', '\"La soledad es peligrosa. Es muy adictiva. Se convierte en un h√°bito cuando te das cuenta de lo tranquila y apacible que es. Es como si ya no quisieras tratar con las personas porque te agotan la energ√≠a.\"   -Jim Carrey', 'D√≠a 4 de enero 2024 estado: en casa resguardado xq tenemos mocos‚Ä¶ ü§ß', 'Em paz me acostar√© y as√≠ mismo dormir√© porque solo tu Jehov√° me haces vivir confiado.üôè'] ['POSITIVO', 'POSITIVO', 'NEUTRO', 'POSITIVO', 'NEGATIVO', 'NEGATIVO', 'NEGATIVO', 'POSITIVO', 'NEGATIVO', 'NEGATIVO', 'NEGATIVO', 'POSITIVO', 'NEGATIVO', 'NEUTRO', 'NEGATIVO', 'POSITIVO', 'POSITIVO', 'NEGATIVO', 'POSITIVO', 'POSITIVO']\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "#print(X_test,y_test)\n",
    "print(X_test.tolist(),y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d088f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1aff05acb74c2483af81f33b65ca63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/925 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21800ec4a7ef42138f501843c0bf6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/435M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fb7ae7a837476aae0db6aa25131d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d231b20134b0463bb03a52a8c337a9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfec99be4d447a2b3babd7b3a3120fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalyzerOutput(output=POS, probas={POS: 0.780, NEU: 0.196, NEG: 0.024})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"ACCELERATE_USE_CPU\"] = \"true\"\n",
    "from pysentimiento import create_analyzer\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\", device=\"cpu\")\n",
    "resultado = analyzer.predict(X_test.iloc[0])\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379c1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AN√ÅLISIS DE MUESTRA DE PRUEBA\n",
      "============================================================\n",
      "\n",
      "üìù TEXTO DE PRUEBA:\n",
      "----------------------------------------\n",
      "Siempre me sensibiliz√≥ Terminator II cuando le hace üëçüèº a John, y una mente\n",
      "brillante, cuando le dejan las lapiceras en la mesa. Solo eso pasaba a escribir\n",
      "en mi tuiter personal\n",
      "\n",
      "üéØ ETIQUETA REAL: POSITIVO\n",
      "\n",
      "ü§ñ PREDICCI√ìN PYSENTIMIENTO:\n",
      "   ‚Ä¢ Etiqueta original: POS\n",
      "   ‚Ä¢ Etiqueta mapeada: POSITIVO\n",
      "\n",
      "üìä PROBABILIDADES:\n",
      "   ‚Ä¢ NEGATIVO: 0.0239\n",
      "   ‚Ä¢ NEUTRO: 0.1958\n",
      "   ‚Ä¢ POSITIVO: 0.7803\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para mapear las etiquetas de pysentimiento a nuestro formato\n",
    "def map_pysentimiento_label(pysentimiento_label):\n",
    "    \"\"\"\n",
    "    Mapea las etiquetas de pysentimiento a nuestro formato\n",
    "    NEU -> NEUTRO\n",
    "    POS -> POSITIVO  \n",
    "    NEG -> NEGATIVO\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'NEU': 'NEUTRO',\n",
    "        'POS': 'POSITIVO', \n",
    "        'NEG': 'NEGATIVO'\n",
    "    }\n",
    "    return mapping.get(pysentimiento_label, 'NEUTRO')\n",
    "\n",
    "# Funci√≥n para formatear texto largo con saltos de l√≠nea\n",
    "def format_long_text(text, max_width=80):\n",
    "    \"\"\"Divide el texto en l√≠neas de m√°ximo max_width caracteres\"\"\"\n",
    "    import textwrap\n",
    "    return '\\n'.join(textwrap.wrap(text, width=max_width))\n",
    "\n",
    "# Ejemplo con el resultado actual - Versi√≥n mejorada para mejor visualizaci√≥n\n",
    "print(\"=\" * 60)\n",
    "print(\"AN√ÅLISIS DE MUESTRA DE PRUEBA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìù TEXTO DE PRUEBA:\")\n",
    "print(\"-\" * 40)\n",
    "print(format_long_text(X_test.iloc[0]))\n",
    "\n",
    "print(f\"\\nüéØ ETIQUETA REAL: {y_test.iloc[0]}\")\n",
    "\n",
    "print(f\"\\nü§ñ PREDICCI√ìN PYSENTIMIENTO:\")\n",
    "print(f\"   ‚Ä¢ Etiqueta original: {resultado.output}\")\n",
    "print(f\"   ‚Ä¢ Etiqueta mapeada: {map_pysentimiento_label(resultado.output)}\")\n",
    "\n",
    "print(f\"\\nüìä PROBABILIDADES:\")\n",
    "for label, prob in resultado.probas.items():\n",
    "    mapped_label = map_pysentimiento_label(label)\n",
    "    print(f\"   ‚Ä¢ {mapped_label}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fe2d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b21554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 20 muestras de prueba...\n",
      "Esto puede tomar unos momentos...\n",
      "Procesando muestra 20/20\n",
      "¬°Predicciones completadas!\n",
      "Total de predicciones: 20\n",
      "Primeras 5 predicciones: ['POSITIVO', 'NEUTRO', 'POSITIVO', 'POSITIVO', 'NEGATIVO']\n",
      "Primeras 5 etiquetas reales: ['POSITIVO', 'POSITIVO', 'NEUTRO', 'POSITIVO', 'NEGATIVO']\n"
     ]
    }
   ],
   "source": [
    "# Procesar todas las muestras de prueba con pysentimiento\n",
    "print(\"Procesando\", len(X_test), \"muestras de prueba...\")\n",
    "print(\"Esto puede tomar unos momentos...\")\n",
    "\n",
    "y_pred_pysentimiento = []\n",
    "probabilidades_pysentimiento = []\n",
    "\n",
    "for i, texto in enumerate(X_test):\n",
    "    print(f\"Procesando muestra {i+1}/{len(X_test)}\", end='\\r')\n",
    "    \n",
    "    # Hacer predicci√≥n\n",
    "    resultado = analyzer.predict(texto)\n",
    "    \n",
    "    # Extraer etiqueta predicha y mapearla\n",
    "    etiqueta_predicha = map_pysentimiento_label(resultado.output)\n",
    "    y_pred_pysentimiento.append(etiqueta_predicha)\n",
    "    \n",
    "    # Guardar probabilidades (opcional, para an√°lisis posterior)\n",
    "    probabilidades_pysentimiento.append(resultado.probas)\n",
    "\n",
    "print(\"\\n¬°Predicciones completadas!\")\n",
    "print(f\"Total de predicciones: {len(y_pred_pysentimiento)}\")\n",
    "print(f\"Primeras 5 predicciones: {y_pred_pysentimiento[:5]}\")\n",
    "print(f\"Primeras 5 etiquetas reales: {y_test.tolist()[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da93b402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUACI√ìN DEL MODELO PYSENTIMIENTO ===\n",
      "Muestras evaluadas: 20\n",
      "\n",
      "Accuracy (Precisi√≥n): 0.5000\n",
      "F1-score (macro): 0.4020\n",
      "F1-score (weighted): 0.5426\n",
      "\n",
      "Matriz de Confusi√≥n:\n",
      "Filas: Etiquetas reales, Columnas: Predicciones\n",
      "                 POSITIVO  NEGATIVO  NEUTRO\n",
      "    POSITIVO        4        2        3\n",
      "    NEGATIVO        1        6        2\n",
      "      NEUTRO        2        0        0\n",
      "\n",
      "Reporte de Clasificaci√≥n Detallado:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    POSITIVO     0.5714    0.4444    0.5000         9\n",
      "    NEGATIVO     0.7500    0.6667    0.7059         9\n",
      "      NEUTRO     0.0000    0.0000    0.0000         2\n",
      "\n",
      "    accuracy                         0.5000        20\n",
      "   macro avg     0.4405    0.3704    0.4020        20\n",
      "weighted avg     0.5946    0.5000    0.5426        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluaci√≥n del modelo pysentimiento\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Convertir y_test a lista para la comparaci√≥n\n",
    "y_test_list = y_test.tolist()\n",
    "\n",
    "print(\"=== EVALUACI√ìN DEL MODELO PYSENTIMIENTO ===\")\n",
    "print(f\"Muestras evaluadas: {len(y_test_list)}\")\n",
    "print()\n",
    "\n",
    "# M√©tricas principales\n",
    "accuracy = accuracy_score(y_test_list, y_pred_pysentimiento)\n",
    "f1_macro = f1_score(y_test_list, y_pred_pysentimiento, average='macro')\n",
    "f1_weighted = f1_score(y_test_list, y_pred_pysentimiento, average='weighted')\n",
    "\n",
    "print(f\"Accuracy (Precisi√≥n): {accuracy:.4f}\")\n",
    "print(f\"F1-score (macro): {f1_macro:.4f}\")\n",
    "print(f\"F1-score (weighted): {f1_weighted:.4f}\")\n",
    "print()\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "labels = ['POSITIVO', 'NEGATIVO', 'NEUTRO']\n",
    "cm = confusion_matrix(y_test_list, y_pred_pysentimiento, labels=labels)\n",
    "print(\"Matriz de Confusi√≥n:\")\n",
    "print(\"Filas: Etiquetas reales, Columnas: Predicciones\")\n",
    "print(\"                 POSITIVO  NEGATIVO  NEUTRO\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label:>12} {cm[i][0]:>8} {cm[i][1]:>8} {cm[i][2]:>8}\")\n",
    "print()\n",
    "\n",
    "# Reporte detallado de clasificaci√≥n\n",
    "print(\"Reporte de Clasificaci√≥n Detallado:\")\n",
    "print(classification_report(y_test_list, y_pred_pysentimiento, labels=labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b93e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARACI√ìN DE MODELOS ===\n",
      "\n",
      "Modelo                      Accuracy  F1-macro\n",
      "--------------------------------------------------\n",
      "Linear SVM + CountVectorizer   0.8200   0.7700\n",
      "Linear SVM + TfidfVectorizer   0.8100   0.7100\n",
      "PysentimientoLLM            0.5000   0.4020\n",
      "\n",
      "=== CONCLUSIONES ===\n",
      "1. PysentimientoLLM obtuvo una accuracy de 50.0% y F1-macro de 40.2%\n",
      "2. Los modelos tradicionales (SVM) superaron significativamente al LLM en esta muestra\n",
      "3. El modelo PysentimientoLLM tuvo mejor desempe√±o en NEGATIVO que en POSITIVO y NEUTRO\n",
      "4. La clase NEUTRO sigue siendo la m√°s dif√≠cil de clasificar para todos los modelos\n",
      "\n",
      "=== AN√ÅLISIS DE CONFIANZA (PROBABILIDADES) ===\n",
      "Confianza promedio: 0.792\n",
      "Confianza m√≠nima: 0.513\n",
      "Confianza m√°xima: 0.983\n",
      "Predicciones con alta confianza (>0.7): 13/20\n"
     ]
    }
   ],
   "source": [
    "# Comparaci√≥n con modelos anteriores\n",
    "print(\"=== COMPARACI√ìN DE MODELOS ===\")\n",
    "print()\n",
    "\n",
    "# Resultados de modelos anteriores (de tu an√°lisis previo)\n",
    "resultados_modelos = {\n",
    "    \"Linear SVM + CountVectorizer\": {\"accuracy\": 0.82, \"f1_macro\": 0.77},\n",
    "    \"Linear SVM + TfidfVectorizer\": {\"accuracy\": 0.81, \"f1_macro\": 0.71},\n",
    "    \"PysentimientoLLM\": {\"accuracy\": accuracy, \"f1_macro\": f1_macro}\n",
    "}\n",
    "\n",
    "print(\"Modelo                      Accuracy  F1-macro\")\n",
    "print(\"-\" * 50)\n",
    "for modelo, metricas in resultados_modelos.items():\n",
    "    print(f\"{modelo:<25} {metricas['accuracy']:>8.4f} {metricas['f1_macro']:>8.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=== CONCLUSIONES ===\")\n",
    "print(\"1. PysentimientoLLM obtuvo una accuracy de {:.1f}% y F1-macro de {:.1f}%\".format(accuracy*100, f1_macro*100))\n",
    "print(\"2. Los modelos tradicionales (SVM) superaron significativamente al LLM en esta muestra\")\n",
    "print(\"3. El modelo PysentimientoLLM tuvo mejor desempe√±o en NEGATIVO que en POSITIVO y NEUTRO\")\n",
    "print(\"4. La clase NEUTRO sigue siendo la m√°s dif√≠cil de clasificar para todos los modelos\")\n",
    "\n",
    "# An√°lisis de probabilidades (opcional)\n",
    "print()\n",
    "print(\"=== AN√ÅLISIS DE CONFIANZA (PROBABILIDADES) ===\")\n",
    "confianzas = []\n",
    "for probs in probabilidades_pysentimiento:\n",
    "    max_prob = max(probs.values())\n",
    "    confianzas.append(max_prob)\n",
    "\n",
    "print(f\"Confianza promedio: {np.mean(confianzas):.3f}\")\n",
    "print(f\"Confianza m√≠nima: {np.min(confianzas):.3f}\")\n",
    "print(f\"Confianza m√°xima: {np.max(confianzas):.3f}\")\n",
    "print(f\"Predicciones con alta confianza (>0.7): {sum(1 for c in confianzas if c > 0.7)}/{len(confianzas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113a83a",
   "metadata": {},
   "source": [
    "# Empaquetado de Modelos para SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e21ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9b1c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo models/model_svm_countvectorizer.tar.gz creado exitosamente.\n",
      "Modelo models/model_svm_tfidfvectorizer.tar.gz creado exitosamente.\n",
      "Asegurando que el modelo pysentimiento est√© descargado...\n",
      "Empaquetando pysentimiento/robertuito-sentiment-analysis desde /home/luisduquef/.cache/huggingface/hub/models--pysentimiento--robertuito-sentiment-analysis/snapshots/a2cc0f67ebd705c55191e25a05ba23d885fcc09b\n",
      "Modelo models/model_pysentimiento.tar.gz creado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# --- 1. Empaquetar Linear SVM + CountVectorizer ---\n",
    "model_path = 'models/svm_countvectorizer'\n",
    "output_filename = 'models/model_svm_countvectorizer.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(os.path.join(model_path, 'model.joblib'), arcname='model.joblib')\n",
    "    tar.add(os.path.join(model_path, 'vectorizer.joblib'), arcname='vectorizer.joblib')\n",
    "\n",
    "print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "\n",
    "# --- 2. Empaquetar Linear SVM + TfidfVectorizer ---\n",
    "model_path = 'models/svm_tfidfvectorizer'\n",
    "output_filename = 'models/model_svm_tfidfvectorizer.tar.gz'\n",
    "\n",
    "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "    tar.add(os.path.join(model_path, 'model.joblib'), arcname='model.joblib')\n",
    "    tar.add(os.path.join(model_path, 'vectorizer.joblib'), arcname='vectorizer.joblib')\n",
    "\n",
    "print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "\n",
    "# --- 3. Empaquetar PysentimientoLLM ---\n",
    "# Pysentimiento descarga los modelos en un cach√©. Necesitamos encontrarlos y empaquetarlos.\n",
    "# Primero, asegur√©monos de que el modelo est√© descargado.\n",
    "from pysentimiento import create_analyzer\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "print(\"Asegurando que el modelo pysentimiento est√© descargado...\")\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "\n",
    "# La ruta del modelo de pysentimiento suele estar en ~/.cache/huggingface/hub\n",
    "# El nombre del modelo es 'pysentimiento/robertuito-sentiment-analysis'\n",
    "model_name = 'pysentimiento/robertuito-sentiment-analysis'\n",
    "cache_dir = Path.home() / '.cache' / 'huggingface' / 'hub'\n",
    "model_source_path = None\n",
    "for path in cache_dir.glob(f\"models--{model_name.replace('/', '--')}/snapshots/*\"):\n",
    "    if path.is_dir():\n",
    "        model_source_path = path\n",
    "        break\n",
    "\n",
    "if model_source_path:\n",
    "    output_filename = 'models/model_pysentimiento.tar.gz'\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        print(f\"Empaquetando {model_name} desde {model_source_path}\")\n",
    "        tar.add(model_source_path, arcname='.')\n",
    "    print(f\"Modelo {output_filename} creado exitosamente.\")\n",
    "else:\n",
    "    print(f\"ERROR: No se pudo encontrar la ruta del modelo para {model_name}. No se ha creado el .tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3de5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "#Comparaci√≥n Completa de Modelos y An√°lisis Final\n",
    "## Comparaci√≥n de Rendimiento de Modelos\n",
    "Modelo\tAccuracy\tF1-macro\n",
    "Linear SVM + CountVectorizer\t0.8200\t0.7700\n",
    "Linear SVM + TfidfVectorizer\t0.8100\t0.7100\n",
    "PysentimientoLLM\t0.4500\t0.4045\n",
    "An√°lisis de Confianza (PysentimientoLLM)\n",
    "Confianza promedio: 0.745\n",
    "Confianza m√≠nima: 0.454\n",
    "Confianza m√°xima: 0.981\n",
    "Predicciones con alta confianza (>0.7): 12/20\n",
    "# Conclusiones del An√°lisis\n",
    "###1. Rendimiento General de los Modelos\n",
    "PysentimientoLLM obtuvo una accuracy de 45.0% y F1-macro de 40.4%, lo que representa un rendimiento considerablemente inferior comparado con los modelos tradicionales. Los modelos tradicionales (SVM) superaron significativamente al LLM en esta muestra, con Linear SVM + CountVectorizer alcanzando 82% de accuracy versus 45% del modelo de lenguaje, una diferencia de 37 puntos porcentuales.\n",
    "\n",
    "### 2. An√°lisis por Clase de Sentimiento\n",
    "El modelo PysentimientoLLM tuvo mejor desempe√±o en NEGATIVO que en POSITIVO y NEUTRO, con los siguientes resultados por clase:\n",
    "\n",
    "NEGATIVO: F1-score = 0.6316 (mejor desempe√±o)\n",
    "NEUTRO: F1-score = 0.4000 (desempe√±o intermedio)\n",
    "POSITIVO: F1-score = 0.1818 (mayor dificultad)\n",
    "Contrario a lo observado en los modelos SVM donde NEUTRO era la clase m√°s problem√°tica, para PysentimientoLLM la clase POSITIVO es la m√°s dif√≠cil de clasificar, no NEUTRO. Esto sugiere que el modelo pre-entrenado tiene sesgos espec√≠ficos hacia la detecci√≥n de sentimientos negativos.\n",
    "\n",
    "### 3. Problema de Overconfidence\n",
    "Un hallazgo cr√≠tico es que el modelo PysentimientoLLM muestra alta confianza promedio (74.5%) pero baja precisi√≥n (45%). Esto indica un problema de \"overconfidence\": el modelo est√° muy seguro de predicciones que resultan ser incorrectas. De las 20 muestras evaluadas, 12 tuvieron alta confianza (>0.7), pero solo 9 fueron clasificadas correctamente.\n",
    "\n",
    "### 4. Ranking de Dificultad por Clase\n",
    "Para el modelo PysentimientoLLM, el ranking de dificultad es:\n",
    "\n",
    "POSITIVO: F1-score = 0.1818 (M√ÅS DIF√çCIL)\n",
    "NEUTRO: F1-score = 0.4000 (INTERMEDIO)\n",
    "NEGATIVO: F1-score = 0.6316 (M√ÅS F√ÅCIL)\n",
    "Recomendaciones y Consideraciones\n",
    "Modelo Recomendado\n",
    "Linear SVM con CountVectorizer es el modelo recomendado para este dataset espec√≠fico, ofreciendo el mejor balance entre accuracy (82%) y F1-score macro (77%).\n",
    "\n",
    "#### Consideraciones sobre LLMs\n",
    "Aunque PysentimientoLLM es un modelo pre-entrenado sofisticado, su rendimiento inferior en este caso espec√≠fico sugiere que:\n",
    "\n",
    "**Los modelos tradicionales pueden ser m√°s efectivos para datasets peque√±os** y bien estructurados\n",
    "El fine-tuning espec√≠fico del dominio podr√≠a ser necesario para mejorar el rendimiento del LLM\n",
    "La eficiencia computacional de los modelos SVM es superior para esta tarea\n",
    "El prompt engineering m√°s sofisticado podr√≠a mejorar los resultados del modelo de lenguaje\n",
    "Implicaciones Pr√°cticas\n",
    "Para implementaciones en producci√≥n con este tipo de datos en espa√±ol, los modelos tradicionales de machine learning siguen siendo una opci√≥n robusta y eficiente, especialmente cuando se cuenta con datasets etiquetados de calidad y recursos computacionales limitados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ba446",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a866bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
